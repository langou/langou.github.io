~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Givens.

Cost of Givens.

derive that G^T

explain the "avoid unnecessary overflow"

Givens vs Householder

(*) first the discussion is not really about Givens vs Householder
but more { 2x2 Householder or 2x2 Givens } vs { Householder }

(*) second Givens to eliminate a column that keep on using the first element
from top to bottom is "the same" as { Householder }. 

(*) a good example to see difference is exercise Problem 21 in Darve and Wooters

(*) Look at the structure of a product of Givens (tridiagonal) 
as opposed to Householder (dense)
then see that (tridiagonal) * (triangular) is (Hessenberg)
then see that (tridiagonal) * (diagonal) is (tridiagonal)

==> if structure on what we will apply to might want to use Givens (or 2x2 Householder)
==> if no structure on what we will apply to, we want to use Househodler: 
less FLOPS, more vectorization

(*) some more comments with parallelization

==> example with Homework 2 question 5
==> example with Tikhonv bidiagonalization algorithm

Fast Givens



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Explain gram-Schmidt
Explain CGS and MGS
Show some implementations

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Homework is CholQR
Go over it slightly though
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
FLOPS QR
comparison Gram Schmidt, Normal Equations (CholQR), Householder
(*) m ≫ n, m = n, m ≥n
(*) do we need Q? which Q do we need (full or reduced)?

m >= n 
   -------------------------------------------------------
   | compacy repres        |  Q and R
----------------------------------------------------------
HH | 2 m n^2 - (2/3) n^3   |  2 x ( 2 m n^2 - (2/3) n^3 )
GS | 2 m n^2               |  2 m n^2
----------------------------------------------------------

m >> n 
   -------------------------------------------------------
   | compacy repres        |  Q and R
----------------------------------------------------------
HH | 2 m n^2               |  4 m n^2
GS | 2 m n^2               |  2 m n^2
----------------------------------------------------------

m = n 
   -------------------------------------------------------
   | compacy repres        |  Q and R
----------------------------------------------------------
HH | (4/3) n^3             |  (8/3) n^3
GS | 2 m n^2               |  2 m n^2
----------------------------------------------------------

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Remarkable results for Householder
there exists an exactly Q s.t. A + E = QR where R is computed R and E is small
how crazy is that?

Even more crazy: true for MGS as well

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Discussion:
(*) cost, performance
(*) stability
(*) do you need the full Q? If yes, how would you do for Gram-Schmidt?
(*) A-inner product

n my mind: 
show some biblio about CholQR2 and the reorthogonalizaton fro Gram-Schmidt

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

SVD: 
give the SVD decomposition and explain the four fundamental spaces of the SVD
(null(A) - range(A) - etc.)
explain how it relates to QR factorization. 
Explain how it relates with UTV.
Explain how it relates with CUR.
Speak about low rank and such.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Least Squares

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Justification with || b - Ax || and then two proofs with QR to end up with
x = R \ ( Q^T b )

Justification of normal equations
either see GVL-4 p.260, top of section 2.3.1

Or get the gradient of "phi"
Two ways to get the gradient: multi-variate calculus or small perturbation

small perturbation
(b-A(x+h))^T * (b-A(x+h)) 
= b^Tb - x^TA^TAx +h^T A^T A x - h^T A^T A b - b^T A^T A h +x^T A^T A h - h^TA^TAh
= (b^Tb - x^TA^TAx) - h^T ( - A^T b - 2 A^T A x ) + h^TA^TAh
Second term is gradient, third term is 1/2 of Hessian




~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Augmented Least Squares
( I  A )(r) = (b)
( A' 0 )(x)   (0)

justification: this is another way to write 
is simply another way of writing the normal equations, A^T Ax = A^Tb.

( I  A )(r) = (b)
( A' 0 )(x)   (0)
==> r + Ax = b   ==> r = b - Ax
==> A'r = 0      ==> A^T ( b - Ax ) = 0

This does not help with rank deficiency of A

The eigenvalues of (the symmetric matrix)
( 0  A )
( A' 0 )
are  ±σᵢ and 0's, so in short the matrix is m+n and we have
n +σᵢ, n -σᵢ, and m-n 0.

The eigenvalues of (the symmetric matrix)
( I  A )
( A' 0 )

Specifically, if σᵢ are the singular values of A, the
eigenvalues \(\lambda \) of \(M\) are the roots of the equation \(\lambda
^{2}-\lambda -σᵢ^{2}=0\) for each σᵢ, along with
additional eigenvalues of 1 and potentially -1 or 0, depending on matrix
dimensions. 



SVD approach: why does it work? 
normal equation approach: why does it work? 

Question from Aurélien about small singular values:
explain that this is really a problem, they blow up any noise
this touches to the fact that pseudoinverse is not continuous

I completely bailed out on this one and say “we do exact arithmetic right now, so assume 0 or not 0, you remove the 0”.
I should have added that there is an essential difficulty here. 
If you want to start removing 1e-14, and keep 1e-10 say, note that what you insert in the formula is a 0 for 1e-14 and 1e+10 for 1e-10.
This is a very difficult task to know what to keep and what to remove, where the threshold is. 
There is a lots of literature on this and this called “regularization”. 

Question from Inès about 
x + null(A) and so we want x ∈ null(A)

For Ax = b or min || b - Ax ||
We understand that, if x is a solution, then x or any x + null(A) is a solution.
If null(A) = {0}, then we recover that the solution is unique.
to get the solution of minimum norm, we want to "remove" the component of x on null(A) 
as follows. 
x = x_{null(A)⟂} + x_{null(A)}
then 
|| x_{null(A)⟂} || is a solution and it is the solution of minimum 2-norm.

For min || b - Ax ||, we want b-Ax ⟂ A.

try to understadn the picture of Darve for Least Square and 1-norm comment
and relate to the picture from matlab webpage for minimum norm:
https://www.mathworks.com/help/matlab/ref/lsqminnorm.html

See homework problem 2.

Fair enough. I mean that if you take a solution x such that A x = b then we can decompose x on Null(A) and Null(A⟂) so we have the decomposition
x = x_{null(A)⟂} + x_{null(A)}
And so _the_ minimum norm solution is x_{null(A)⟂}, so we remove what "remove Null(A) from x”. 
I agree that I did not mathematically explained this well. So thanks to Inès. That was fair. 
So this shows that x ∈ null(A)⟂
But we know that null(A)⟂ is Range(Aᵀ) so the minimum norm solution is such Ax = b and x ∈ Range(Aᵀ).
So x = Aᵀ y for some y. So we want AAᵀ y = b, we assumed invertibility so y = ( AAᵀ )⁻¹b so x = A ( AAᵀ )⁻¹ b
That’s one way to get the minimum norm solution. (We can call it “normal equation” method.)




~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
minimum norm problem
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Generalizing the problem
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We are now looking for THE minimum norm least squares solution

QR will not work, one need SVD or COD


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pseudo inverse in all this
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
recap various formulas

why we like pseudo-inverse: existence and unicity
a least squares problem has a solution, 
and if this solution is not unique, then
we can have unicity with minimum norm solution

random thoughts:
A\b for minimum norm in Matlab vs Julia (Matlab needs lsqminnorm)

Explain the pictures in Darve and Wooters, and also the one on lsqminnorm page of mathworks

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Condition number of Least Squares
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
I still do not get the same results as Trefethen LOL
But I think I get the same one as 
Also, do some experiments to show that the bound is tight
This is delayed to after the break

Question: 
(*) what would be a condition number for the LS problem? (the definition)
(*) what would be a backward error for the LS problem? (the definition)
(*) what would be a stability result for a LS problem algorithm? (the definition)

