~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

(*) explain fundamental subspace of a matrix with SVD

pseudo-inverse

(*) review homework3

(*) show lecturez/lecturez.txt on condition number of linear least squares

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Eigenvalue computation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Presentation of the players
(*) SEP, NEP, SVD
Explain the three fundamental decompositions

Compute the full decomposition. A is dense.

For NEP, explain Schur. Why Schur and not A = X Œõ X‚Åª¬π? (Or A = X J X‚Åª¬π?)

Intrisic difficulty for NEP. (Diagonalization does not exist.)
Difference with SEP and SVD. This is why we go to Orthogonal Schur form.
Explain that the set of diagonalizable matrices is dense in the set of all matrices

A = X Œõ X‚Åª¬π    
	A X = X Œõ

A = X Œõ Y·¥¥  (where Y·¥¥ = X‚Åª¬π and so that Y·¥¥ X = I)

A  = Y‚Åª·¥¥ Œõ Y·¥¥ 
	Y·¥¥ A  = Œõ Y·¥¥ 
	X‚Åª¬π A = Œõ X‚Åª¬π    

A·µÄ = Y‚Åª¬π Œõ Y  (where we conjugate Œõ but complex-conjugate, so kind of the same)
A·µÄ = X·¥¥ Œõ X‚Åª·¥¥  
	‚ö†Ô∏è  A·µÄ X·¥¥ = X·¥¥ Œõ ‚ö†Ô∏è 

Note: other variants 
(**) compute a few of the eigenvalues, extremal, interior
(**) A is sparse (or data sparse)
(**) notably hamiltonian matrix
(**) notably generalized eigenvalue problem Ax = Œª Bx
(**) notably polynomial eigenvalue problem (M Œª¬≤+ C Œª + K) x = 0
(**) GSVD (Generalized SVD), CS Decomposition
(**) product eigenvalue problem
(**) ùìû(n¬≤) algorithm for companion matrices

Unicity? 
	Av = Œªv, for a given Œª, is v unique?
	Then look at decompositions.

Specifity of ‚Ñù for NEP.
	Œª = a ¬± i b       x = u ¬± i v
	Look at invariant subspace of dimension 2 spanned by [u,v] in ‚Ñù‚Åø
	=> [u,v] span the same invariant space in ‚ÑÇ‚Åø as [u ¬± i v]

Why work in ‚Ñù and not in ‚ÑÇ?
(**) One multiplication in ‚ÑÇ is four multiplication in ‚Ñù 
(**) size of  
(**) stability, structure of the problem (complex conjugate eigenvalues)

Reordering of the Schur form for NEP

Standard form for NEP is
	[ a c; d a ]
( we would prefer [ a b; -b a ] with b > 0 but not stable )
See homework problem #3.

‚ùì How many eigenvalues of a random matrix are real?
Alan Edelman, Eric Kostlan, Michael Shub
How Many Eigenvalues of a Random Matrix are Real?
Journal of the American Mathematical Society, Vol. 7, No. 1 (Jan., 1994), pp. 247-267 (21 pages)
https://doi.org/10.2307/2152729

No "direct" computation but algorithms that converge quadratically or cubically
        (**) Abel-Ruffini theorem, n ‚â• 5

Bonjour!

Le th√©or√®me d'Abel-Ruffini affirme qu'il n'existe pas de formule g√©n√©rale par radicaux, i.e. avec les quatres op√©rations, les puissances enti√®res et les racines n-i√®mes. Mais on peut trouver des formules exactes en utilisant d'autres formes.

Cet article donne une solution g√©n√©rale sous forme de s√©rie en utilisant les nombres de Catalan.
https://www.tandfonline.com/doi/full/10.1080/00029890.2025.2460966

Bonne journ√©e,
Cl√©ment

	(**) explain what linear, quadratic or cubic convergence mean
	(**) det( A - Œª I ) is not a good method
	(**) explain how matlab compute roots of polynomial
[ companion matrix ]
[   0 1 0 . . . . . 0        ]
[   0 0 1 . . . . . 0        ]
[                            ]
[                0  1        ]
[ -a0 -a1 -a2 . . . -a_{n-1} ]

p(x) = x^n + a_{n-1} x^{n-1} + . . . + a1 x + a0

Two-sided transformations

High level for algorithms:
(1) go to condensed form
(2) iterative algorithm
(3) optionally reorder and/or compute eigenvectors (NEP)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A = X Œõ Y·¥¥ 

Rayleigh quotient: definition. Why?

‚ùì What is a good shift for Inverse Iteration? Do you see a problem with it?

 ‚öôÔ∏è  include("Power_Iteration.jl")
 ‚öôÔ∏è  include("Inverse_Iteration.jl")
 ‚öôÔ∏è  include("Inverse_Iteration_BigFloat.jl")
 ‚öôÔ∏è  include("Rayleigh_Quotient.jl")  -- picture with sphere
 ‚öôÔ∏è  include("Rayleigh_Quotient_BigFloat.jl")

 ‚öôÔ∏è  include("Orthogonal_Iteration.jl")

explain convergence of Rayleigh Quotient Iteration

[ example symmetric matrices ]
1e-1 for v => 1e-2 for lambda
so (1e-1 for v) * (1e-2 for lambda) => (1e-3) for v
1e-3 for v => 1e-6 for lambda
so (1e-3 for v) * (1e-6 for lambda) => (1e-9) for v

But do explain (and with some code) that for shift and invert when mu is close to lambda we have:
so (1e-3 for v) * (1e-7 for lambda) => (1e-10) for v [ for example ]


‚ùì 
