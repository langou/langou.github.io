~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Homework 1 bien reÃ§u

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Homework 2 --> dÃ» pour mardi prochain

Doit on changer le deadline? Peut Ãªtre que 7:59am les mardi matin n'est pas optimal

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Higham â€œWhat isâ€ articles
https://nhigham.com/index-of-what-is-articles/
https://nhigham.com/2022/04/28/what-is-a-permutation-matrix/
https://nhigham.com/2020/07/14/what-is-the-growth-factor-for-gaussian-elimination/
https://nhigham.com/2020/08/04/what-is-numerical-stability/
https://nhigham.com/2020/03/25/what-is-backward-error/
https://nhigham.com/2020/07/21/what-is-a-symmetric-positive-definite-matrix/
https://nhigham.com/2020/08/11/what-is-a-cholesky-factorization/
https://nhigham.com/2020/09/15/what-is-a-householder-matrix/
https://nhigham.com/2020/11/10/what-is-a-qr-factorization/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Julia and HPC

Frontier (AMD), #2 in Top 500 2025, Oak Ridge National Lab
Aurora (Intel), #3 in Top 500 2025, Argonne National Lab

The main reason is that a code (for example) Krylov.jl is polymorphic, that is
to say, the same implementation can be efficient for many different types.

Support for many different precisions (Float16, Float32, Float64, Float128). 

But also support for several architectures (CPU / GPU) thanks to array typing
(Vector{Float64}, CuVector{Float64}, ROCVector{Float64}).

In addition, we can easily support GPUs in Julia thanks to JIT (just-in-time)
compilation to LLVM, which is quite similar between the CPU and GPU.

On AMD, there is the AMDGPU.jl interface and we just added methods for
ROCArrays for LinearAlgebra.dot, LinearAlgebra.axpy, etc., which call ROCBLAS.

Thanks to the multiple dispatch, Krylov.jl was then able to function smoothly
on AMD GPUs.

For Intel, it was a bit more complicated because oneMKL didn't have a C
interface, so we couldn't call Intel's routines. We therefore created our own
GPU kernels using KernelAbstractions.jl and then added the LinearAlgebra
methods (which call our custom kernels) that we needed for the oneArray.

In both cases, with the right GPU support and multiple dispatch, we were able
to solve linear systems on the Frontier and Aurora supercomputers.

Almost no work compared to a basic reimplementation of all Krylov methods in
C++ with Intel kernels (Intel GPUs) or ROCBLAS calls (AMD GPUs).

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

orthogonal in â„
unitary in â„‚

â€¼ï¸  In LAPACK, ZUNGQR and DORGQR
â€¼ï¸  Akin to BLAS, ZHEMM and DSYMM

orthonormal columns
orthogonal columns
QQá´´ is not Qá´´Q


QR factorization

(---) Definition 
	(---) 3 properties
        (---) two types: reduced and full
        (---) two cases: m â‰¥ n and m â‰¤ n

ğŸ§® main_geqrf_01.jl

(---) Methods
	(---) Gram Schmidt â¯â¯â¯â¯ ğŸ› ï¸ orthogonal projections
	(---) Cholesky QR  â¯â¯â¯â¯ ğŸ› ï¸ normal equations
	(---) Householder  â¯â¯â¯â¯ ğŸ› ï¸ Householder elementary reflection (unitary transformation)
	(---) Givens       â¯â¯â¯â¯ ğŸ› ï¸ Givens rotation (unitary transformation)

ğŸ§© https://en.wikipedia.org/wiki/JÃ¸rgen_Pedersen_Gram
ğŸ§© https://en.wikipedia.org/wiki/Erhard_Schmidt
ğŸ§© https://en.wikipedia.org/wiki/Alston_Scott_Householder
ğŸ§© https://en.wikipedia.org/wiki/Wallace_Givens

(---) Existence and Uniqueness

   Existence by construction with the Gram Schmidt algorithm
  â“ to understand uniqueness, how can it not be unique?

we will say "essentially unique"

(---) Application for a Linear Least Squares
 ğŸ‘‰ with the thin, with the full
 â“ would another norm be better? why 2-norm?

ğŸ§® homework02_01.jl

 â“ why: Q = Matrix(Q)
 â“ why: F.Q*I or collect(F.Q)

 ğŸ”œ what is the condition number of min \| A x - b \|_2 ?
 ğŸ”œ stability of algorithms?

(---) Application for finding minimum norm solution

ğŸ§® homework02_02.jl

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

elementary Householder reflector


(---) build larfg!()
(---) start with projection, build elementary Householder reflector, construct v
(---) in â„, two reflectors, which one do we pick?


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ğŸ§® main_geqrf_02.jl
ğŸ§® main_geqrf_03.jl

   ( I - v * Ï„ * v' ) * x
     = x - v * Ï„ * v' * x

( parentheses matter, order of operation matters )

Discrete Fourier Transform II, DCT II

ğŸ§® main_geqrf_04.jl (with LAPACK)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

cost

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

LARFGP
Parlett's trick

   ğŸ“‘ James W. Demmel, Mark Hoemmen, Yozo Hida, and E. Jason Riedy
      Nonnegative Diagonals and High Performance on Low-Profile Matrices from Householder QR
      SIAM Journal on Scientific Computing
      Vol. 31, Iss. 4 (2009)
      https://doi.org/10.1137/080725763

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


random thoughts:
(---) some variants: QR, LQ, QL and RQ

âš™ï¸  LAPACK âš™ï¸  geqrf
âš™ï¸  LAPACK âš™ï¸  orgqr, ungqr
âš™ï¸  LAPACK âš™ï¸  ormqr, unmqr

âš™ï¸  LAPACK âš™ï¸  gels

âš™ï¸  LAPACK âš™ï¸  larfg
âš™ï¸  LAPACK âš™ï¸  larf
âš™ï¸  LAPACK âš™ï¸  larft
âš™ï¸  LAPACK âš™ï¸  larfb

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  â“ lots of possible choices for the vector v and Ï„ in particular in â„‚

     ğŸ‘‰ having ones on the diagonal of V makes sense since we have specialized kernels for handling triangular matrices

     ğŸ‘‰ it seems to be mainly historical or arbritrary reasons

     ğŸ“‘ R.B. Lehoucq. The computation of elementary unitary matrices. ACM Trans. Math. Software, 22 (4) (1996), pp. 393-400
        â— LINPACK
        â— EISPACK
        â— NAG
        â— LAPACK

     ğŸ” in â„‚, can we recompute Ï„ from v? We can do it in â„.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


Blocking with WY 

QR with pivoting
low rank factorization
randomized algorithm

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

