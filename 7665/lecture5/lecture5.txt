~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Higham â€œWhat isâ€ articles
https://nhigham.com/index-of-what-is-articles/
https://nhigham.com/2022/04/28/what-is-a-permutation-matrix/
https://nhigham.com/2020/07/14/what-is-the-growth-factor-for-gaussian-elimination/
https://nhigham.com/2020/08/04/what-is-numerical-stability/
https://nhigham.com/2020/03/25/what-is-backward-error/
https://nhigham.com/2020/07/21/what-is-a-symmetric-positive-definite-matrix/
https://nhigham.com/2020/08/11/what-is-a-cholesky-factorization/
https://nhigham.com/2020/09/15/what-is-a-householder-matrix/
https://nhigham.com/2020/11/10/what-is-a-qr-factorization/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 â— For Homework #1: Cholesky: do not forget to start with a SPD matrix!
 â— Also, I could share the lapack codes if desired.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Julia and HPC

Frontier (AMD), #2 in Top 500 2025, Oak Ridge National Lab
Aurora (Intel), #3 in Top 500 2025, Argonne National Lab

The main reason is that a code (for example) Krylov.jl is polymorphic, that is
to say, the same implementation can be efficient for many different types.

Support for many different precisions (Float16, Float32, Float64, Float128). 

But also support for several architectures (CPU / GPU) thanks to array typing
(Vector{Float64}, CuVector{Float64}, ROCVector{Float64}).

In addition, we can easily support GPUs in Julia thanks to JIT (just-in-time)
compilation to LLVM, which is quite similar between the CPU and GPU.

On AMD, there is the AMDGPU.jl interface and we just added methods for
ROCArrays for LinearAlgebra.dot, LinearAlgebra.axpy, etc., which call ROCBLAS.

Thanks to the multiple dispatch, Krylov.jl was then able to function smoothly
on AMD GPUs.

For Intel, it was a bit more complicated because oneMKL didn't have a C
interface, so we couldn't call Intel's routines. We therefore created our own
GPU kernels using KernelAbstractions.jl and then added the LinearAlgebra
methods (which call our custom kernels) that we needed for the oneArray.

In both cases, with the right GPU support and multiple dispatch, we were able
to solve linear systems on the Frontier and Aurora supercomputers.

Almost no work compared to a basic reimplementation of all Krylov methods in
C++ with Intel kernels (Intel GPUs) or ROCBLAS calls (AMD GPUs).

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

orthogonal in â„
unitary in â„‚

â€¼ï¸  In LAPACK, ZUNGQR and DORGQR
â€¼ï¸  Akin to BLAS, ZHEMM and DSYMM

orthonormal columns
orthogonal columns
QQá´´ is not Qá´´Q


QR factorization

(---) Definition (3 things)

(---) two types: thin and full

(---) two cases: m â‰¥ n and m â‰¤ n

ğŸ§® main_geqrf_01.jl

(---) Methods
	(---) Gram Schmidt â¯â¯â¯â¯ ğŸ› ï¸ orthogonal projections
	(---) Cholesky QR  â¯â¯â¯â¯ ğŸ› ï¸ normal equations
	(---) Householder  â¯â¯â¯â¯ ğŸ› ï¸ Householder elementary reflection (unitary transformation)
	(---) Givens       â¯â¯â¯â¯ ğŸ› ï¸ Givens rotation (unitary transformation)

ğŸ§© https://en.wikipedia.org/wiki/JÃ¸rgen_Pedersen_Gram
ğŸ§© https://en.wikipedia.org/wiki/Erhard_Schmidt
ğŸ§© https://en.wikipedia.org/wiki/Alston_Scott_Householder
ğŸ§© https://en.wikipedia.org/wiki/Wallace_Givens

(---) Existence and Uniqueness

   Existence by construction with the Gram Schmidt algorithm
  â“ to understand uniqueness, how can it not be unique?

we will say "essentially unique"

(---) Application for a Linear Least Squares
 ğŸ‘‰ with the thin, with the full
 â“ would another norm be better? why 2-norm?

ğŸ§® homework02_01.jl

 â“ why: Q = Matrix(Q)

 ğŸ”œ what is the condition number of min \| A x - b \|_2 ?
 ğŸ”œ stability of algorithm?

(---) Application for finding minimum norm solution

ğŸ§® homework02_02.jl

random thoughts:
(---) some variants: QR, LQ, QL and RQ

âš™ï¸  LAPACK âš™ï¸  geqrf
âš™ï¸  LAPACK âš™ï¸  orgqr
âš™ï¸  LAPACK âš™ï¸  ormqr

âš™ï¸  LAPACK âš™ï¸  gels

âš™ï¸  LAPACK âš™ï¸  larfg
âš™ï¸  LAPACK âš™ï¸  larf
âš™ï¸  LAPACK âš™ï¸  larft
âš™ï¸  LAPACK âš™ï¸  larfb

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   ( I - v * Ï„ * v' ) * x
     = x - v * Ï„ * v' * x

( parenthesis matter )



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  â“ lots of possible choices for the vector v and Ï„

     ğŸ‘‰ having ones on the diagonal of V makes sense since we have specialized kernels for handling triangular matrices

     ğŸ‘‰ it seems to be mainly historical or arbritrary reasons

     ğŸ“‘ R.B. Lehoucq. The computation of elementary unitary matrices. ACM Trans. Math. Software, 22 (4) (1996), pp. 393-400
        â— LINPACK
        â— EISPACK
        â— NAG
        â— LAPACK

     ğŸ” in â„‚, can we recompute Ï„ from v? We can do it in â„.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


Blocking with WY 



QR with pivoting, low rank factorization
randomized algorithm

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

